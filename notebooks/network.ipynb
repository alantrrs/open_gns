{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNS Implementation Details\n",
    "\n",
    "The model works by adopting a particle-based representation of the physical system. Physical dynamics are approximated by interactions among the particles. The objective of the model is to learn these interactions.\n",
    "\n",
    "## Input and output representations\n",
    "### Inputs\n",
    "Each particle's input state vector represents:\n",
    "- Position, $p_i^{t_k}$\n",
    "- A sequence of $C=5$ previous velocities. The velocity is calculated from the difference in position between consecutive time steps: $\\dot{p}^{t_k}=p^{t_k}-p^{t_{k-1}}$\n",
    "- Features that capture the static material properties (e.g. water, sand, rigid, etc..). The material is expressed as a particle feature, $a_i$, represented with a learned embedding vector of size 16.\n",
    "- The global properties of the system, $g$, include external forces and global material properties.\n",
    "\n",
    "$$x^{t_k}_i = [p^{t_k}_i,\\dot{p}^{t_k-C+1}_i,...,\\dot{p}^{t_k}_i,f_i]$$\n",
    "\n",
    "- For datasets with fixed flat orthogonal walls, instead of adding boundary particles, a feature is added to each node indicating the vector distance to each wall. To maintain spatial transalation invariance, this distance is clipped to the connectivity radius $R$, achieving a similar effect to that of the boundary particles.\n",
    "\n",
    "### Outputs\n",
    "The prediction targets for supervised learning are the per-particle average acceleration, $$\\ddot{p}_i=\\dot{p}^{t_k+1}-\\dot{p}^{t_k}=p^{t_{k+1}}-2p^{t_k}+p^{t_{k-1}}$$\n",
    "\n",
    "\n",
    "## Encoder\n",
    "The encoder embeds the particle-based state representation, $X$, as a latent graph $G_0=\\text{ENCODER}(X)$, where $G=(V,E,\\mathbf{u})$, $\\mathbf{v}_i\\in V$, and $\\mathbf{e}_{i,j}\\in E$.\n",
    "- The encoder constructs the graph structure $G^0$ by assignning a node to each particle and adding edges between particles within a connectivity radius, $R$. On each timestep the graph's edges are recomputed by a nearest neighbor algorithm, implemented by a standard kd-tree, to reflect the current particle positions.\n",
    "- The node embeddings, $\\mathbf{v}_i=\\varepsilon ^v(x_i)$, are learned functions of the particles' states.\n",
    "- The edge embeddings, $\\mathbf{e}_{i,j}=\\varepsilon^e(\\mathbf{r}_{i,j})$, are learned functions of the pairwise properties of the corresponding particles, $\\mathbf{r}_{i,j}$, e.g., displacement between their positions, spring constant, etc.\n",
    "- $\\varepsilon^v$ and $\\varepsilon^e$ as a multilayer perceptron, which encode node features and edge features into the latent vectors, $v_i$ and $e_{i,j}$, of size $128$.\n",
    "- The graph-level embedding, $\\mathbf{u}$, could represent global properties such as gravity and magnetic fields. Althoguh, this is currently implemented as node level features instead.\n",
    "\n",
    "## Processor\n",
    "...\n",
    "\n",
    "## Decoder\n",
    "...\n",
    "\n",
    "## Noise\n",
    "...TODO...\n",
    "\n",
    "## Normalization\n",
    "...TODO...\n",
    "\n",
    "## Loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "Parameters for BoxBath:\n",
    "- Trajectory length: 150\n",
    "- Number of rollouts: Train/Validation/Test -> 2700/150/150\n",
    "- Connectivity radius: $R=0.08$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
