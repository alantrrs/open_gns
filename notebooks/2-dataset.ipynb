{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and output representations\n",
    "## Inputs\n",
    "Each particle's input state vector represents:\n",
    "- Position, $p_i^{t}$\n",
    "- A sequence of $C=5$ previous velocities. The velocity is calculated from the difference in position between consecutive time steps: $\\dot{p}^t=p^t-p^{t-1}$\n",
    "- Features that capture the static material properties (e.g. water, sand, rigid, etc..). The material is expressed as a particle feature, $a_i$, represented with a learned embedding vector of size 16.\n",
    "- The global properties of the system, $g$, include external forces and global material properties.\n",
    "- For datasets with fixed flat orthogonal walls, instead of adding boundary particles, a feature is added to each node indicating the vector distance to each wall, $d^{t}_i$. To maintain spatial transalation invariance, this distance is clipped to the connectivity radius $R$, achieving a similar effect to that of the boundary particles.\n",
    "\n",
    "The particle feature tensor looks as follows:\n",
    "$$x^{t}_i = [p^{t}_i,\\dot{p}^{t-C+1}_i,...,\\dot{p}^{t}_i,a_i, g, d^{t}_i]$$\n",
    "\n",
    "\n",
    "## Outputs\n",
    "The prediction targets for supervised learning are the per-particle average acceleration, $$\\ddot{p}^t_i=\\dot{p}^{t+1}-\\dot{p}^t=p^{t+1}-2p^{t}+p^{t-1}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%cd /workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile open_gns/dataset.py\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.transforms import RadiusGraph\n",
    "\n",
    "R=0.08 # Connectivity radius $R$\n",
    "\n",
    "class GNSDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, split='train'):\n",
    "        if split == 'train':\n",
    "            self.split_idx = 0\n",
    "        elif split == 'validation':\n",
    "            self.split_idx = 1\n",
    "        elif split == 'test':\n",
    "            self.split_idx = 2\n",
    "        else:\n",
    "            raise ValueError(f'Split {split} not found.')\n",
    "        super(GNSDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data_file = h5py.File(self.processed_paths[0], 'r')\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [f'{self.root}/box_bath.hdf5']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        splits = ('train', 'val', 'test')\n",
    "        return [f'box_bath_{splits[self.split_idx]}.hdf5']\n",
    "    \n",
    "    def process_rollout(self, positions):\n",
    "        data_list = []\n",
    "        num_steps = len(positions)\n",
    "        # Calculate velocities\n",
    "        velocities = np.concatenate(([np.zeros(positions[0].shape)],\n",
    "                                    positions[1:] - positions[0:-1]),axis=0)\n",
    "        # Calculate accelerations\n",
    "        accelerations = np.concatenate(([np.zeros(velocities[0].shape)],\n",
    "                                    velocities[1:] - velocities[0:-1]),axis=0)\n",
    "        # Material properties (using one-hot encoding for now)\n",
    "        m = np.zeros((len(positions[0]), 2))\n",
    "        m[0:64] = [0,1] # First 64 particles are solid\n",
    "        m[64:] = [1,0]\n",
    "        # TODO: Global forces\n",
    "        # Drop the first 5 and the last step since we don't have accurate velocities/accelerations\n",
    "        for t in range(6,num_steps-1):\n",
    "            # Distance to the 5 walls\n",
    "            d = np.stack([\n",
    "                positions[t][:,1],       # bottom\n",
    "                positions[t][:,0],       # left\n",
    "                positions[t][:,2],        # back\n",
    "                1.2 - positions[t][:,0], # right\n",
    "                0.4 - positions[t][:,2]   # front\n",
    "            ], axis=1)\n",
    "            d = np.clip(d, 0, R)\n",
    "            x = np.concatenate((positions[t], m, np.concatenate(velocities[t-5:t], axis=1), d), axis=1)\n",
    "            y = torch.tensor(accelerations[t]).float()\n",
    "            data = Data(x=torch.tensor(x).float(), y=y, pos=torch.as_tensor(positions[t]))\n",
    "            # print(f'Step {t}:', data)\n",
    "            # Apply pre-transform to get edges\n",
    "            calculate_edges = self.pre_transform or RadiusGraph(R)\n",
    "            data = calculate_edges(data)\n",
    "            data_list.append(data)\n",
    "        return data_list\n",
    "    \n",
    "    def save_data_to_hdf5(self, output_file, data_list, start_index=0):\n",
    "        for i, data in enumerate(data_list):\n",
    "            idx = start_index + i\n",
    "            for k in ['x', 'edge_index', 'pos', 'y']:\n",
    "                output_file.create_dataset(f'data/{idx}/{k}', data=getattr(data, k), compression='gzip')\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.data_file.get('data').keys())\n",
    "    \n",
    "    def get(self, index):\n",
    "        d = self.data_file[f'data/{index}']\n",
    "        data = Data(\n",
    "            x=torch.as_tensor(np.array(d['x'])),\n",
    "            edge_index=torch.as_tensor(np.array(d['edge_index'])),\n",
    "            pos=torch.as_tensor(np.array(d['pos'])),\n",
    "            y=torch.as_tensor(np.array(d['y'])))\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "        # Read all positions & transform into features\n",
    "        f = h5py.File(self.raw_file_names[0],'r')\n",
    "        out = h5py.File(self.processed_paths[0], 'w')\n",
    "        rollouts = [range(1000), range(1000,1100), range(1100,1200)]\n",
    "        start_index = 0\n",
    "        for rollout in rollouts[self.split_idx]:\n",
    "            print('Rollout:', rollout)\n",
    "            positions = np.array(f.get(f'rollouts/{rollout}/positions'))\n",
    "            try:\n",
    "                data_list = self.process_rollout(positions)\n",
    "                self.save_data_to_hdf5(out, data_list, start_index=start_index)\n",
    "                start_index += len(data_list)\n",
    "            except Exception as e:\n",
    "                print(f'Bad rollout? {rollout}: {e}')\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from open_gns.dataset import GNSDataset\n",
    "\n",
    "dataset = GNSDataset('./notebooks', split='validation')\n",
    "# Validating the data\n",
    "data = dataset[0]\n",
    "print(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
